{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ewu2023/CS589-HW4/blob/main/CS589_NeuralNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg570F-XxICR"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEWGG2M_xNfo",
        "outputId": "8a365142-375f-472a-ac98-861fb6560d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B-JBg83M1ok"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WsRlxN3fLmfw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOW3NjBRNpsU"
      },
      "source": [
        "# Define Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PlC7io6JNYw8"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self, networkShape, trainData: pd.DataFrame, classLabels, weights=None, regParam=0, alpha=0.01, debugFlag=False):\n",
        "        \"\"\"\n",
        "        Constructor for the neural network class.\n",
        "        \n",
        "        networkShape: A list of integers that contains the number of neurons to use in each layer\n",
        "\n",
        "        trainData: The data set that will be used to train the model\n",
        "\n",
        "        weights: A list of weight matrices for each hidden layer. If initialized to None, the constructor will assign random weights\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Process training data\n",
        "        \"\"\"\n",
        "        self.trainData = trainData\n",
        "\n",
        "        \n",
        "        # Get a copy of the training data without the labels\n",
        "        self.noLabelTrainData = trainData.loc[:, ~trainData.columns.isin(classLabels)]\n",
        "        self.classLabels = classLabels\n",
        "\n",
        "        # Encode class vectors\n",
        "        self.classVectors = {}\n",
        "        for i in range(len(trainData)):\n",
        "            # Get current row from training data\n",
        "            row = trainData.iloc[i]\n",
        "\n",
        "            # Iterate over all classes and assign values\n",
        "            classVector = {}\n",
        "            for label in classLabels:\n",
        "                expVal = row[label]\n",
        "                classVector[label] = [expVal]\n",
        "\n",
        "            # Convert class vector to numpy array\n",
        "            classVecDf = pd.DataFrame(classVector)\n",
        "\n",
        "            # Append class vector to dictionary of vectors\n",
        "            # Implementation uses column vectors, so we take transpose here\n",
        "            self.classVectors[i] = (classVecDf.to_numpy()).T\n",
        "\n",
        "        self.networkShape = networkShape\n",
        "\n",
        "        # Set the value of the regularization parameter\n",
        "        self.regParam = regParam\n",
        "\n",
        "        # Set the step size for gradient descent\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Set debug flag to show output of intermediate computations\n",
        "        self.debugFlag = debugFlag\n",
        "\n",
        "        # Instance variable storing weight matrices\n",
        "        self.layers = []\n",
        "        \n",
        "        # Initialize layers\n",
        "        for i in range(len(networkShape) - 1):\n",
        "            # Get the number of neurons in the current and next layers\n",
        "            numCurLayer = networkShape[i] + 1 # Account for neurons + bias term in current layer\n",
        "            numNextLayer = networkShape[i + 1]\n",
        "\n",
        "            # Initialize matrix for the current layer\n",
        "            # Number of rows = number of neurons in layer i + 1\n",
        "            # Number of columns = number of neurons in layer i\n",
        "            layerMatrix = np.zeros(shape=(numNextLayer, numCurLayer))\n",
        "            if weights:\n",
        "                layerMatrix = weights[i]\n",
        "            else:\n",
        "                self._init_matrix(layerMatrix)\n",
        "            \n",
        "            # Append current layer to the list of layers\n",
        "            self.layers.append(layerMatrix)\n",
        "        \n",
        "    def _init_matrix(self, matrix: np.ndarray):\n",
        "        rows, cols = matrix.shape\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                matrix[i, j] = norm.rvs()\n",
        "    \n",
        "    # Definition for the sigmoid function\n",
        "    def sigmoid(self, x):\n",
        "        return (1 / (1 + np.exp(-x)))\n",
        "    \n",
        "    # Compute activation vector\n",
        "    def compute_activation_vector(self, weightedSums: np.ndarray):\n",
        "        numRows, numCols = weightedSums.shape\n",
        "        activationVector = np.zeros(shape=(numRows, numCols))\n",
        "\n",
        "        for i in range(numRows):\n",
        "            # Get weighted sum from i-th row\n",
        "            x = weightedSums[i, 0]\n",
        "\n",
        "            # Compute output of sigmoid function and place it in activation vector\n",
        "            activationVector[i, 0] = self.sigmoid(x)\n",
        "        \n",
        "        return activationVector\n",
        "    \n",
        "    # Method for propagating forward one instance\n",
        "    def propagate_one(self, instance, activations=None):\n",
        "        # Add a bias term to the instance\n",
        "        instanceAsNP = instance.to_numpy()\n",
        "        instanceVector = np.concatenate(([1], instanceAsNP))\n",
        "        \n",
        "        # Make instance vector a column vector\n",
        "        instanceVector = np.atleast_2d(instanceVector).T\n",
        "        \n",
        "        # Iterate over each layer and compute activations for each neuron\n",
        "        prevActivation = instanceVector # Keep track of the activation vector for previous layer\n",
        "\n",
        "        if activations != None: # If activations is not None, append current activation\n",
        "            activations.append(prevActivation)\n",
        "\n",
        "        # If the debug flag was set, print out the first instance vector\n",
        "        if self.debugFlag:\n",
        "            print(f\"Value of a0:\\n{prevActivation}\\n\")\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            # Get current weight matrix\n",
        "            curTheta = self.layers[i]\n",
        "\n",
        "            # Compute weighted sum vector (z-matrix): Theta^{l=i-1} * a^{l=i-1}\n",
        "            z = np.matmul(curTheta, prevActivation)\n",
        "\n",
        "            # Compute activation vector of current layer\n",
        "            curActivationVec = self.compute_activation_vector(z)\n",
        "            \n",
        "            # Add bias term to current activation vector\n",
        "            curActivationVec = np.concatenate(([[1]], curActivationVec)) # Prepend 1 to vector\n",
        "\n",
        "            # Update previous activation vector\n",
        "            prevActivation = curActivationVec\n",
        "\n",
        "            # Append current activation to list\n",
        "            if activations != None:\n",
        "                activations.append(curActivationVec)\n",
        "\n",
        "            # Print results of computation at this step if debug flag is on\n",
        "            if self.debugFlag:\n",
        "                print(f\"Value of z{i + 1}:\\n{z}\")\n",
        "                print(f\"Value of a{i + 1}:\\n{curActivationVec}\\n\")\n",
        "        \n",
        "        # Compute activation at the final layer\n",
        "        lastTheta = self.layers[len(self.layers) - 1]\n",
        "        lastZMat = np.matmul(lastTheta, prevActivation)\n",
        "        outputVector = self.compute_activation_vector(lastZMat)\n",
        "\n",
        "        # If the debug flag was set, print results of this final computation\n",
        "        if self.debugFlag:\n",
        "            print(f\"Value of z{len(self.layers)}:\\n{lastZMat}\")\n",
        "            print(f\"Value of a{len(self.layers)}:\\n{outputVector}\\n\")\n",
        "\n",
        "        if activations != None:\n",
        "            activations.append(outputVector)\n",
        "\n",
        "        # Return as a vector in the event there are multiple outputs\n",
        "        return outputVector\n",
        "\n",
        "    # Compute error for an individual output of the neural network\n",
        "    def compute_one_instance_err(self, expVal, predVal):\n",
        "        return -expVal * np.log(predVal) - (1 - expVal) * np.log(1 - predVal)\n",
        "\n",
        "    # Helper method for computing regularized error\n",
        "    def compute_error(self):\n",
        "        # Keep track of total error across all training instances\n",
        "        totalErr = 0\n",
        "\n",
        "        # Iterate over all training instances\n",
        "        for i in range(len(self.noLabelTrainData)):\n",
        "            # Get current instance and perform forward propagation on it\n",
        "            curInstance = self.noLabelTrainData.iloc[i]\n",
        "            predVector = self.propagate_one(curInstance) # Will be a vector\n",
        "\n",
        "            # Get the expected values vector\n",
        "            expVector = self.classVectors[i]\n",
        "\n",
        "            # Compute error vector\n",
        "            vectorizedErrorFunc = np.vectorize(self.compute_one_instance_err)\n",
        "            errVector = vectorizedErrorFunc(expVector, predVector)\n",
        "\n",
        "            # Sum all elements of error vector, then add it to total error\n",
        "            totalErr += np.sum(errVector)\n",
        "        \n",
        "        # Compute average error\n",
        "        avgErr = totalErr / len(self.trainData)\n",
        "\n",
        "        \"\"\" Compute the squared sum of all weights in the network \"\"\"\n",
        "        weightSqSum = 0\n",
        "        for weightMatrix in self.layers:\n",
        "            # Square all of the weights\n",
        "            squaredMatrix = np.multiply(weightMatrix, weightMatrix)\n",
        "\n",
        "            # Add all columns, then add each column's total to get the total sum for this matrix\n",
        "            colSums = np.sum(squaredMatrix, axis=0)\n",
        "            matrixTotal = np.sum(colSums)\n",
        "\n",
        "            # Add matrix total to sum of the weights squared\n",
        "            weightSqSum += matrixTotal\n",
        "        \n",
        "        # Regularize error\n",
        "        weightSqSum *= (self.regParam / (2 * len(self.trainData)))\n",
        "\n",
        "        # Return error + regularization term\n",
        "        return avgErr + weightSqSum\n",
        "    \n",
        "    def backpropagate(self):\n",
        "        # Initialize gradients for each layer\n",
        "        # Accumulate the gradients in this list\n",
        "        gradients = []\n",
        "        for layer in self.layers:\n",
        "            # Get the shape of each layer\n",
        "            rows, cols = layer.shape\n",
        "            gradients.append(np.zeros(shape=(rows,cols)))\n",
        "\n",
        "\n",
        "        # Iterate over all instances in the training data\n",
        "        for i in range(len(self.noLabelTrainData)):\n",
        "            print(f\"--- Propagating Instance {i} ---\\n\")\n",
        "\n",
        "            # Propagate current instance through the network\n",
        "            curInstance = self.noLabelTrainData.iloc[i]\n",
        "            activations_i = []\n",
        "            self.propagate_one(curInstance, activations=activations_i)\n",
        "\n",
        "            # Compute delta values for output layer\n",
        "            outputVector = activations_i[len(activations_i) - 1]\n",
        "            if self.debugFlag:\n",
        "                print(f\"Instance {i} Activation: {outputVector.T}\\n\")\n",
        "\n",
        "            expectedVector = self.classVectors[i]\n",
        "\n",
        "            delta_vectors = []\n",
        "            delta_vectors.append(outputVector - expectedVector)\n",
        "\n",
        "            # Iterate over each layer and compute delta values\n",
        "            for k in range(len(self.layers) - 1, 0, -1):\n",
        "                # Get current weight matrix\n",
        "                weightMatrix = self.layers[k]\n",
        "                delta_next = delta_vectors[0]\n",
        "\n",
        "                # Compute delta values for nodes in current layer\n",
        "                a = np.matmul(weightMatrix.T, delta_next)\n",
        "                b = np.multiply(a, activations_i[k])\n",
        "                delta_k = np.multiply(b, (1 - activations_i[k]))\n",
        "\n",
        "                # Remove bias term from delta_k\n",
        "                delta_k = np.delete(delta_k, 0, 0)\n",
        "\n",
        "                # Append to delta vectors\n",
        "                delta_vectors.insert(0, delta_k)\n",
        "\n",
        "            \n",
        "            # Accumulate the gradients for each layer\n",
        "            if self.debugFlag:\n",
        "                print(f\"Instance {i} Deltas:\\n\")\n",
        "                for deltaIndex in range(len(delta_vectors)):\n",
        "                    print(f\"Deltas for Layer {deltaIndex}:\\n{delta_vectors[deltaIndex]}\\n\")\n",
        "            \n",
        "            for j in range(len(self.layers) - 1, -1, -1):\n",
        "                curGradLayer = gradients[j]\n",
        "\n",
        "                # Compute gradients of this instance for current layer\n",
        "                gradMatrix = np.matmul(delta_vectors[j], activations_i[j].T)\n",
        "                if self.debugFlag:\n",
        "                    print(f\"Gradients of Theta{j} on instance {i}:\\n{gradMatrix}\\n\")\n",
        "\n",
        "                # Accumulate the gradients\n",
        "                curGradLayer = curGradLayer + gradMatrix\n",
        "                gradients[j] = curGradLayer\n",
        "\n",
        "        # Iterate over each layer and compute gradient + regularization factor\n",
        "        for i in range(len(self.layers) - 1, -1, -1):\n",
        "            regFactor = np.multiply(self.regParam, self.layers[i])\n",
        "            # Set first col of regFactor to 0s\n",
        "            rows, cols = regFactor.shape\n",
        "            regFactor[:,0] = np.zeros(rows)\n",
        "\n",
        "            curGradLayer = gradients[i]\n",
        "\n",
        "            curGradLayer = (1 / len(self.trainData)) * (curGradLayer + regFactor)\n",
        "            gradients[i] = curGradLayer\n",
        "\n",
        "        # Print final gradients\n",
        "        if self.debugFlag:\n",
        "            print(\"----------\")\n",
        "            print(f\"Final Average, Regularized Gradients:\\n\")\n",
        "            for gradIndex in range(len(gradients)):\n",
        "                print(f\"Final Regularized gradients of Theta{gradIndex}:\\n{gradients[gradIndex]}\\n\")\n",
        "\n",
        "        # Update weights according to gradients\n",
        "        for i in range(len(self.layers) - 1, -1, -1):\n",
        "            self.layers[i] = self.layers[i] - self.alpha * gradients[i]\n",
        "        \n",
        "        # Testing: Print all layers\n",
        "        # if self.debugFlag:\n",
        "        #     for layer in self.layers:\n",
        "        #         print(f\"{layer}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxzyGtabHfub"
      },
      "source": [
        "Test Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8_y6PpYnHiGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d149d1-ebb6-4237-b1fc-2139dd6ce76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Propagating Instance 0 ---\n",
            "\n",
            "Value of a0:\n",
            "[[1.  ]\n",
            " [0.32]\n",
            " [0.68]]\n",
            "\n",
            "Value of z1:\n",
            "[[0.74  ]\n",
            " [1.1192]\n",
            " [0.3564]\n",
            " [0.8744]]\n",
            "Value of a1:\n",
            "[[1.        ]\n",
            " [0.67699586]\n",
            " [0.75384029]\n",
            " [0.5881687 ]\n",
            " [0.70566042]]\n",
            "\n",
            "Value of z2:\n",
            "[[1.94769138]\n",
            " [2.12135808]\n",
            " [1.48153575]]\n",
            "Value of a2:\n",
            "[[1.        ]\n",
            " [0.87519469]\n",
            " [0.89296181]\n",
            " [0.81480444]]\n",
            "\n",
            "Value of z3:\n",
            "[[1.60830969]\n",
            " [1.66804824]]\n",
            "Value of a3:\n",
            "[[0.83317658]\n",
            " [0.84131543]]\n",
            "\n",
            "Instance 0 Activation: [[0.83317658 0.84131543]]\n",
            "\n",
            "Instance 0 Deltas:\n",
            "\n",
            "Deltas for Layer 0:\n",
            "[[-0.00086743]\n",
            " [-0.00133354]\n",
            " [-0.00053312]\n",
            " [-0.00070163]]\n",
            "\n",
            "Deltas for Layer 1:\n",
            "[[ 0.00638937]\n",
            " [-0.00925379]\n",
            " [-0.00778767]]\n",
            "\n",
            "Deltas for Layer 2:\n",
            "[[ 0.08317658]\n",
            " [-0.13868457]]\n",
            "\n",
            "Gradients of Theta2 on instance 0:\n",
            "[[ 0.08317658  0.0727957   0.07427351  0.06777264]\n",
            " [-0.13868457 -0.121376   -0.12384003 -0.1130008 ]]\n",
            "\n",
            "Gradients of Theta1 on instance 0:\n",
            "[[ 0.00638937  0.00432557  0.00481656  0.00375802  0.00450872]\n",
            " [-0.00925379 -0.00626478 -0.00697588 -0.00544279 -0.00653003]\n",
            " [-0.00778767 -0.00527222 -0.00587066 -0.00458046 -0.00549545]]\n",
            "\n",
            "Gradients of Theta0 on instance 0:\n",
            "[[-0.00086743 -0.00027758 -0.00058985]\n",
            " [-0.00133354 -0.00042673 -0.00090681]\n",
            " [-0.00053312 -0.0001706  -0.00036252]\n",
            " [-0.00070163 -0.00022452 -0.00047711]]\n",
            "\n",
            "--- Propagating Instance 1 ---\n",
            "\n",
            "Value of a0:\n",
            "[[1.  ]\n",
            " [0.83]\n",
            " [0.02]]\n",
            "\n",
            "Value of z1:\n",
            "[[0.5525]\n",
            " [0.8138]\n",
            " [0.1761]\n",
            " [0.6041]]\n",
            "Value of a1:\n",
            "[[1.        ]\n",
            " [0.63471542]\n",
            " [0.69291867]\n",
            " [0.54391158]\n",
            " [0.64659376]]\n",
            "\n",
            "Value of z2:\n",
            "[[1.81695963]\n",
            " [2.02468436]\n",
            " [1.373268  ]]\n",
            "Value of a2:\n",
            "[[1.        ]\n",
            " [0.86020091]\n",
            " [0.88336451]\n",
            " [0.79790763]]\n",
            "\n",
            "Value of z3:\n",
            "[[1.58227893]\n",
            " [1.64577265]]\n",
            "Value of a3:\n",
            "[[0.82952703]\n",
            " [0.83831889]]\n",
            "\n",
            "Instance 1 Activation: [[0.82952703 0.83831889]]\n",
            "\n",
            "Instance 1 Deltas:\n",
            "\n",
            "Deltas for Layer 0:\n",
            "[[0.01694006]\n",
            " [0.01465141]\n",
            " [0.01998824]\n",
            " [0.01622017]]\n",
            "\n",
            "Deltas for Layer 1:\n",
            "[[0.01503437]\n",
            " [0.05808969]\n",
            " [0.06891698]]\n",
            "\n",
            "Deltas for Layer 2:\n",
            "[[0.07952703]\n",
            " [0.55831889]]\n",
            "\n",
            "Gradients of Theta2 on instance 1:\n",
            "[[0.07952703 0.06840922 0.07025135 0.06345522]\n",
            " [0.55831889 0.48026642 0.4931991  0.44548691]]\n",
            "\n",
            "Gradients of Theta1 on instance 1:\n",
            "[[0.01503437 0.00954254 0.01041759 0.00817737 0.00972113]\n",
            " [0.05808969 0.03687042 0.04025143 0.03159565 0.03756043]\n",
            " [0.06891698 0.04374267 0.04775386 0.03748474 0.04456129]]\n",
            "\n",
            "Gradients of Theta0 on instance 1:\n",
            "[[0.01694006 0.01406025 0.0003388 ]\n",
            " [0.01465141 0.01216067 0.00029303]\n",
            " [0.01998824 0.01659024 0.00039976]\n",
            " [0.01622017 0.01346274 0.0003244 ]]\n",
            "\n",
            "----------\n",
            "Final Average, Regularized Gradients:\n",
            "\n",
            "Final Regularized gradients of Theta0:\n",
            "[[0.00803632 0.02564134 0.04987447]\n",
            " [0.00665894 0.01836697 0.06719311]\n",
            " [0.00972756 0.03195982 0.05251862]\n",
            " [0.00775927 0.05036911 0.08492365]]\n",
            "\n",
            "Final Regularized gradients of Theta1:\n",
            "[[0.01071187 0.09068406 0.02511708 0.1259677  0.11586492]\n",
            " [0.02441795 0.06780282 0.04163777 0.05307643 0.1267652 ]\n",
            " [0.03056466 0.08923522 0.1209416  0.10270214 0.03078292]]\n",
            "\n",
            "Final Regularized gradients of Theta2:\n",
            "[[0.0813518  0.17935246 0.12476243 0.13186393]\n",
            " [0.20981716 0.19194521 0.30342954 0.25249305]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def backprop_example_1():\n",
        "    d = {\n",
        "        'x': [0.13000, 0.42000], \n",
        "        'class': [0.90000, 0.23000]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data=d)\n",
        "    df_noLabels = df.loc[:, df.columns!='class']\n",
        "\n",
        "    networkShape = [1, 2, 1]\n",
        "    weights = [\n",
        "        np.array(\n",
        "            [[0.40000, 0.10000],\n",
        "            [0.30000, 0.20000]],\n",
        "        ),\n",
        "\n",
        "        np.array([[0.70000, 0.50000, 0.60000]])\n",
        "    ]\n",
        "\n",
        "    classLabels = ['class']\n",
        "    network = NeuralNetwork(networkShape, trainData=df, classLabels=classLabels, weights=weights, debugFlag=True)\n",
        "    network.backpropagate()\n",
        "\n",
        "def backprop_example_2():\n",
        "    # Pre-process data\n",
        "    d = {\n",
        "        \"x1\": [0.32000, 0.83000],\n",
        "        \"x2\": [0.68000, 0.02000],\n",
        "        \"y1\": [0.75000, 0.75000],\n",
        "        \"y2\": [0.98000, 0.28000]\n",
        "    }\n",
        "\n",
        "    classLabels = [\"y1\", \"y2\"]\n",
        "\n",
        "    df = pd.DataFrame(d)\n",
        "    df_noLabels = df.loc[:, ~df.columns.isin(classLabels)]\n",
        "    \n",
        "    # Initialize network\n",
        "    networkShape = [2, 4, 3, 2]\n",
        "    weights = [\n",
        "        np.array([\n",
        "            [0.42000, 0.15000, 0.40000],\n",
        "            [0.72000, 0.10000, 0.54000],\n",
        "            [0.01000, 0.19000, 0.42000],\n",
        "            [0.30000, 0.35000, 0.68000]\n",
        "        ]),\n",
        "\n",
        "        np.array([\n",
        "            [0.21000, 0.67000, 0.14000, 0.96000, 0.87000],\n",
        "            [0.87000, 0.42000, 0.20000, 0.32000, 0.89000],\n",
        "            [0.03000, 0.56000, 0.80000, 0.69000, 0.09000]\n",
        "        ]),\n",
        "\n",
        "        np.array([\n",
        "            [0.04000,  0.87000,  0.42000,  0.53000],\n",
        "            [0.17000,  0.10000,  0.95000,  0.69000]\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    network = NeuralNetwork(\n",
        "        networkShape, \n",
        "        df, \n",
        "        classLabels=classLabels, \n",
        "        weights=weights, \n",
        "        regParam=0.25, \n",
        "        debugFlag=True\n",
        "    )\n",
        "\n",
        "    network.backpropagate()\n",
        "\n",
        "backprop_example_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JObkuUYH0ATB"
      },
      "source": [
        "# Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wQR5HQY0BkY",
        "outputId": "b109e03f-1a77-4f07-f1b5-db933105a4b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4 6]\n",
            "[3 7]\n",
            "Sum of all elements in matrix: 10\n",
            "b-vector elements squared:\n",
            "[[ 9]\n",
            " [49]]\n",
            "[[ 6]\n",
            " [14]]\n"
          ]
        }
      ],
      "source": [
        "# target_dir = \"/content/drive/My Drive/\"\n",
        "# filename = \"blarg.json\"\n",
        "# fileRoute = f\"{target_dir}/{filename}\"\n",
        "\n",
        "# with open(fileRoute, 'w') as outfile:\n",
        "#     some_data = {\n",
        "#         \"a1\": [[1, 2], [3, 4]],\n",
        "#         \"b1\": [[5, 6], [7, 8]]\n",
        "#     }\n",
        "\n",
        "#     outfile.write(json.dumps(some_data))\n",
        "\n",
        "# Compute error for an individual output of the neural network\n",
        "def compute_one_instance_err(expVal, predVal):\n",
        "    return -expVal * np.log(predVal) - (1 - expVal) * np.log(1 - predVal)\n",
        "\n",
        "def multiply(a, b):\n",
        "    return a * b\n",
        "\n",
        "\n",
        "vectorFunction = np.vectorize(multiply)\n",
        "a = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4]\n",
        "])\n",
        "\n",
        "colSum = np.sum(a, axis=0)\n",
        "rowSum = np.sum(a, axis=1)\n",
        "\n",
        "print(colSum)\n",
        "print(rowSum)\n",
        "print(f\"Sum of all elements in matrix: {np.sum(colSum)}\")\n",
        "\n",
        "b = np.array([\n",
        "    [3],\n",
        "    [7]\n",
        "])\n",
        "\n",
        "print(f\"b-vector elements squared:\\n{np.multiply(b, b)}\")\n",
        "# print(f\"b-vector - b-vector: {b - b}\")\n",
        "print(f\"{2 * b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Pandas Dummies"
      ],
      "metadata": {
        "id": "VJruN2ffcmiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"temperature\": [\"hot\", \"cool\", \"hot\"],\n",
        "    \"weather\": [\"sunny\", \"overcast\", \"sunny\"],\n",
        "    \"humidity\": [\"high\", \"normal\", \"high\"],\n",
        "    \"class\": [\"no\", \"yes\", \"no\"]\n",
        "}\n",
        "\n",
        "tennisDf = pd.DataFrame(data)\n",
        "encodedDf = pd.get_dummies(tennisDf)\n",
        "for i in range(len(encodedDf)):\n",
        "    print(f\"{encodedDf.iloc[i]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CylZyQ3cn4n",
        "outputId": "e52f385b-0ed8-4b01-d566-b15ad2d1fb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temperature_cool    0\n",
            "temperature_hot     1\n",
            "weather_overcast    0\n",
            "weather_sunny       1\n",
            "humidity_high       1\n",
            "humidity_normal     0\n",
            "class_no            1\n",
            "class_yes           0\n",
            "Name: 0, dtype: uint8\n",
            "\n",
            "temperature_cool    1\n",
            "temperature_hot     0\n",
            "weather_overcast    1\n",
            "weather_sunny       0\n",
            "humidity_high       0\n",
            "humidity_normal     1\n",
            "class_no            0\n",
            "class_yes           1\n",
            "Name: 1, dtype: uint8\n",
            "\n",
            "temperature_cool    0\n",
            "temperature_hot     1\n",
            "weather_overcast    0\n",
            "weather_sunny       1\n",
            "humidity_high       1\n",
            "humidity_normal     0\n",
            "class_no            1\n",
            "class_yes           0\n",
            "Name: 2, dtype: uint8\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzSuoqm+EQ1SdgKs8Ccvgd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}